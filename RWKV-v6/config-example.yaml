# -----
#
#  infctx 训练器基于 PyTorch Lightning，并使用以下 YAML 配置文件格式。
# 对于本示例中未记录的许多训练器参数/设置，可以在 PyTorch Lightning 文档中找到更多详细信息：
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.cli.LightningCLI.html
#
# 为了更好地配置特定数据集，你可能需要查看 `notebook/dataset-config` 中的各种笔记本。
#
# -----

# 使用整数值来启用固定的训练种子。
seed_everything: true
trainer:
  # 配置你机器上可用的 GPU 数量。
  # auto 表示自动检测并使用所有 GPU。
  accelerator: gpu
  devices: auto
  num_nodes: 1

  #
  # 配置 DeepSpeed 策略，建议你从 `deepspeed_stage_2_offload` 开始，
  # 然后根据你的训练需求进行调整。`deepspeed_stage_3_offload` 对于在单个 GPU 上训练大型模型的 LoRA 很有用。
  #
  # 通常你会希望使用以下策略：
  #
  # - deepspeed_stage_1 : 如果你的每个 GPU 都有太多显存，且你不知道该怎么做。
  #
  # - deepspeed_stage_2 : 最佳分布式训练策略，适用于每个 GPU 都有足够显存的情况。
  # - deepspeed_stage_2_offload : 通过将优化器状态和工作卸载到 CPU 来减少显存使用。
  #
  # - deepspeed_stage_3 : 将模型拆分到多个 GPU 上，适用于大型模型，但会带来性能损失。
  # - deepspeed_stage_3_offload : 额外的卸载，进一步增加性能损失。
  #
  # 更多详细信息请参阅：
  # https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed-zero-stage-2
  #
  strategy: deepspeed_stage_2_offload

  # 模型的浮点精度，因为 RWKV 是为 bf16 构建的，
  # 所以你几乎不应该更改此设置。
  precision: bf16

  # 用于 wandb 的日志记录器设置，如果你想启用 wandb，请取消注释整个 logger 部分。
  # ---
  # logger:
  #   class_path: lightning.pytorch.loggers.WandbLogger
  #   init_args:
  #     # 在大多数情况下，你只需要修改 name/project/tags，
  #     # 或者运行 ID / 恢复标志。
  #     name: null
  #     project: RWKV_training
  #     tags: ['RWKV']
  #     id: null
  #     resume: null
  #
  #     # 其余是高级 WANDB 设置，在大多数情况下你可以删除/忽略。
  #     save_dir: .
  #     version: null
  #     offline: false
  #     dir: null
  #     anonymous: null
  #     log_model: false
  #     experiment: null
  #     prefix: ''
  #     checkpoint_name: null
  #     job_type: null
  #     config: null
  #     entity: null
  #     reinit: null
  #     group: null
  #     notes: null
  #     magic: null
  #     config_exclude_keys: null
  #     config_include_keys: null
  #     mode: null
  #     allow_val_change: null
  #     force: null
  #     tensorboard: null
  #     sync_tensorboard: null
  #     monitor_gym: null
  #     save_code: null
  #     settings: null
  
  # 训练过程的检查点设置。
  callbacks:
    class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      # 配置你想要保存检查点的路径。
      # 注意，将创建一个子目录，名称为 `epoch=x-step=y.ckpt`。
      #
      # 要将检查点转换为模型，你可以使用
      # `python3 export_checkpoint.py <checkpoint path>` 脚本，
      # 它将在检查点目录中创建一个 `rwkv_model.pth`。
      #
      # 不要使用 `zero_to_fp32.py` 脚本，因为它会导致导出格式问题。
      dirpath: /path/to/your/checkpoint/dir
      filename: null
      
      # 保存 top/last K 个检查点。
      save_top_k: 3
      # 按步骤选择最近的检查点。
      monitor: 'step'
      mode: max
      
      # 如果启用（true），将最新的检查点保存为 'last.ckpt'。
      # 这对于简化检查点恢复脚本很有用，但会牺牲磁盘性能。
      save_last: true

      # 不要将此设置为 true，因为导出的模型权重会有格式问题。
      # 请使用 `export_checkpoint.py` 脚本将检查点转换为模型。
      save_weights_only: false

      # 你希望每隔多少步保存一次检查点。
      # 这将发生在每 X 个数据样本之后，其中 X = every_n_train_steps * accumulate_grad_batches。
      #
      # 通常你会希望避免设置一个较低的数字（特别是如果 accumulate_grad_batches <= 100），
      # 因为检查点过程会暂停所有 GPU 训练一段时间，从而减慢整体进程。
      # 但你也不希望设置一个过高的数字，因为如果训练崩溃，你会失去太多进度。
      every_n_train_steps: 100
      every_n_epochs: null
      save_on_train_epoch_end: true
      train_time_interval: null

      # 其他 PyTorch Lightning 设置，在大多数情况下你可以删除/忽略。
      # ---
      # verbose: false
      # auto_insert_metric_name: true
  
  ########################################
  ## 训练运行参数设置
  ########################################

  # 通常你需要配置的是最大 epoch 数。
  # 将其设置为 -1，它将一直运行直到被中断。
  # 或者将其设置为一个数字，它将在该 epoch 数后停止。
  max_epochs: 1
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null

  # 每个步骤训练的数据样本数，一个数据样本被视为
  # wandb 日志中的“子步骤”，而“步骤”则被跟踪为“trainer/global_step”。
  #
  # 这决定了在反向传播之前，从多少个数据样本中一起学习。
  #
  # `1 trainer/global_step = accumulate_grad_batches * GPU 设备数量 * 节点数量`
  #
  # 建议设置为足够大的数字（如 128/256），以防止训练损失在过程中波动。
  # 但不要设置过大的数字，因为增加的 GPU 显存 / 卸载的 RAM 使用可能会导致训练崩溃。
  #
  # 你还建议将此配置为足够大的数字，以充分利用
  # 你的 GPU 处理时间百分比，并避免 GPU 在批次之间的空闲时间。
  target_batch_size: 32

  # 我们将数据分块处理的微批次大小，这会显著增加每个 GPU 步骤的显存使用，
  # 但会显著提高训练过程的吞吐量。
  #
  # 因此，如果你每个 GPU 每批次有 16 个数据样本，且微批次大小为 2，则你有 8 个子步骤。
  #
  # 通常建议将其调整为你 GPU 可以合理支持的最高值，
  # 因为它直接影响你的整体 tokens/秒 计数。
  #
  # 通常你先调整 microbatch_size，然后再调整 target_batch_size。
  microbatch_size: 1

  # 你可以直接设置每个 GPU 的 accumulate_grad_batches。
  # （不推荐）
  #
  # 你只能使用 target_batch_size，它会根据你拥有的 GPU 数量自动计算此值，
  # 或者直接设置此值——不能同时使用两者。
  #
  # `target_batch_size ~= GPU 数量 * accumulate_grad_batches`
  #
  # 如果 target_batch_size 不能被 GPU 数量“完美整除”，
  # 此数字将向下取整，最小值为 1。
  # ---
  # accumulate_grad_batches: 256

  # 其他各种 PyTorch Lightning 设置，在大多数情况下你可以删除/忽略。
  # ---
  # fast_dev_run: false
  # limit_train_batches: null
  # limit_val_batches: null
  # limit_test_batches: null
  # limit_predict_batches: null
  # overfit_batches: 0.0
  # val_check_interval: null
  # check_val_every_n_epoch: 1
  # num_sanity_val_steps: 0
  # log_every_n_steps: 1
  # enable_checkpointing: null
  # enable_progress_bar: null
  # enable_model_summary: null
  # gradient_clip_val: 1.0
  # gradient_clip_algorithm: null
  # deterministic: null
  # benchmark: null
  # inference_mode: true
  # use_distributed_sampler: true
  # profiler: null
  # detect_anomaly: false
  # barebones: false
  # plugins: null
  # sync_batchnorm: false
  # reload_dataloaders_every_n_epochs: 0
  # default_root_dir: null

########################################
## 训练模型设置
########################################
model:
  # 用于开始微调/训练过程的模型。
  load_model: /path/to/your/model.pth

  # 训练过程中使用的上下文长度。
  # 数字越大（以及批次大小越大），显存使用量越大。
  #
  # 注意，如果数据样本的上下文长度大于 ctx_len，
  # 其训练过程将被拆分为 ctx_len 大小的块。
  #
  # 这允许训练非常大的上下文长度（例如 100k），
  # 而不会通过保持训练上下文长度为适合当前 GPU 设置的合理数字来消耗太多显存。
  ctx_len: 2048

  # 训练过程的学习率。
  # ---
  # 过程的初始学习率。
  lr_init: 6e-4
  # 学习率周期结束后的最终学习率。
  # 学习率将从此保持在最终值。
  #
  # 注意：lr_final / lr_period 不适用于 warmup_steps，
  #       并且将被忽略（或被 warmup_steps 逻辑替换）。
  lr_final: 4e-4
  # 将学习率从 lr_init 降低到 lr_final 的 epoch 数。
  #  1 表示单个 epoch（因此从 epoch 2 开始，学习率将为 lr_final）。
  #  0 表示 lr_final 将立即应用。
  # -1 表示我们将当前 max_step / max_epoch 作为周期。
  lr_period: -1
  # 如果设置了 lr_period 类型，默认为 epoch。
  lr_period_type: epoch

  # 通过时间反向传播，用于解决超出当前 GPU 显存架构支持的大上下文长度训练问题。
  #
  # 这与使用完整显存的相同训练过程不完全相同，
  # 因为训练过程被拆分为多个部分，逐段进行。
  # 每个部分的学习有限。
  bptt_learning: true

  # 执行反向传播学习的分段范围。
  # 1 表示仅应用于最后一段。
  # -1 表示应用于所有段。
  #
  # 对于多 GPU 训练，尽可能使用固定值以减少 GPU 同步开销。
  # 当使用固定数据集上下文长度时，对于混合数据集大小，-1 是一个合理的折衷。
  bptt_learning_range: -1

  # 将 bptt 学习限制为仅“当前”块
  # 在学习范围内。虽然这降低了 bptt 的有效性，
  # 但它也进一步减少了显存需求。
  #
  # 这也称为 tbptt（截断通过时间反向传播）。
  bptt_truncated_learning: false

  # 在每个数据样本之间积极清除 CUDA 缓存。
  # 这会导致性能损失，但会减少显存压力。
  #
  # 这对于缓解以下显存压力警告很有用：
  # `自上次步骤以来，1 次 PyTorch 分配器缓存刷新。这发生在显存压力高时，对性能不利...`
  substep_cuda_cache_clear: false

  # 模型大小设置，用于当前模型。
  # 建议不要配置此设置，而是使用自动检测。
  # 否则，这对于在运行前仔细检查模型设置很有用。
  #
  # 有关当前模型设置的详细信息，请参阅下载模型的模型卡。
  # ---
  # n_embd: 768
  # n_layer: 12
  # vocab_size: 50277

  # 实验性截断设置。
  # ---
  # 如果数据样本大于 ctx_len，则将其截断为相应的 max ctx_len_cutoffs 值。
  # 如果数据样本大于最大的 len_cutoff，则丢弃剩余数据。
  #
  # 将其保留为空数组以禁用此功能。
  # ---
  # ctx_len_cutoffs: []
  # ---
  # 实验性设置，数据样本前缀中要跳过的 token 数，
  # 用于加速过程。
  #
  # 将其保留为空数组以禁用此功能。
  # ---
  # ctx_len_warmup_steps: []

  # torch.set_float32_matmul_precision，用于优化张量核心操作。
  # 对于非 CUDA 核心 GPU，应将其设置为 null。据我所知，没有重大影响。
  # ---
  # torch_set_float32_matmul_precision: 'high'

  # Adam 优化器设置。
  # 你可能希望保留此设置不变，除非你知道自己在做什么。
  # ---
  # beta1: 0.9
  # beta2: 0.99
  # adam_eps: 1.0e-08
  # weight_decay: 0.01

  # 其他各种 PyTorch Lightning 设置，你可能应该保留不变。
  # ---
  # grad_cp: true
  # warmup_steps: -1
  # layerwise_lr: true
  # dim_att: null
  # dim_ffn: null
data:
  # 跳过数据路径设置。
  #
  # 如果使用 preload_datapath.py，则忽略此设置，对于加速训练器启动很有用，
  # 前提是你已正确预初始化所有数据集。
  # ---
  # skip_datapath_setup: True

  # 要使用的数据包配置 yaml，这将覆盖以下所有设置。
  # ---
  # datapack_config_path: null

  # 预构建数据集的 dataset_path，使用 HF `load_from_disk()`。
  #
  # 如果你已构建自己的数据集并使用 `save_to_disk()` 保存，
  # 且 source 为 null，则使用此设置。否则，将此配置为 Huggingface 数据集过程将构建和标记化的目录。
  #
  # 如果使用相对路径，则此路径应相对于训练器脚本路径。
  data_path: /path/to/store/your/data_path/

  # 数据路径存储选项，用于通过 Huggingface 数据集 API 支持云存储。请参阅：
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # 注意：截至 2023 年 1 月，这些选项仅测试过与 AWS S3 和 Backblaze 的兼容性。YMMV。
  #       对于 S3 存储桶支持，你还需要安装 s3fs `python3 -m pip install s3fs`。
  #
  # 如果你想减少意外提交密钥/秘密的风险，可以使用
  # `AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 环境变量。
  #
  # 对于 datapath，应使用 `s3://bucket-name/subpath` 格式。
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>
  
  # 否则提供 source 路径，这是 Huggingface 数据集路径，
  # 将用于填充 dataset_path。
  #
  # 使用以下之一：
  # - Huggingface 数据集
  # - 本地数据集模式（例如：text,json,csv - 然后使用 source_data_dir 配置路径）
  # - null
  #
  # 如果 source 被禁用，则除 data_path 外的所有参数实际上都被忽略，
  # 因为数据集构建过程被跳过。你需要自己准备符合 Huggingface 的数据集，
  # 并将其保存到 data_path。
  source: null
  # source: "teven/enwiki_00k"   # Huggingface 数据集
  # source: text                 # 文本模式，与 source_data_dir 一起使用。

  # 从 HF 数据集中使用的数据集拆分。
  # ---
  # source_dataset_split: train

  # 额外的源数据集参数，用于获取数据集的子集。
  # ---
  # source_dataset_params:
  #   language: en

  # 按长度对数据集进行排序，有助于减少 GPU 等待时间（也有助于 RWKV 长上下文一致性）。
  sort_by_length: false
  sort_asc: true # 按升序排序，true = 最短优先，false = 最长优先

  # 限制文档数量，设置为偏移量/长度限制。
  # 如果使用整数值，则解释为文档数量。
  # 如果使用浮点值（<1.0），则解释为数据集的百分比。
  # ---
  # dataset_offset: -1
  # dataset_length: -1

  # 如果使用 source=text/json/etc，则使用 data_dir。
  # 如果使用相对路径，则此路径应相对于训练器脚本路径。
  # source_data_dir: ../dataset-text/

  # 加载数据集后，拆分出用于验证的测试数据，
  # 如果数据集包含测试拆分，则跳过此过程。
  #
  # 如果给定浮点值，则使用数据集的百分比（1.0 表示 100%）。
  # 如果给定整数值，则使用数据样本的数量。
  #
  # 由于训练器过程的限制，始终至少有 1 个测试样本。
  test_split: 0.01
  test_split_shuffle: true

  # 使用的标记器，使用内置的 'neox' 或 'world' 标记器。
  # 如果使用自定义标记器，请提供 HF 标记器名称/路径。
  # ---
  tokenizer: world

  # 数据集使用的最小/最大 token 大小。
  # 用于从大型数据集中过滤出小的噪声数据样本
  # （例如，从维基百科中删除少于 1024 个 token 的小文章）。
  #
  # 如果设置为 -1，则忽略此设置。
  # ---
  # min_token_size: 1024
  # max_token_size: -1

  # 使用的自定义文本列，适用于具有替代训练列标签的数据集。
  # 此设置在多列合并之前检查，默认为 null（禁用）。
  # 如果设置，则优先。
  # 例如：'code'
  # ---
  # custom_text_key: 'code'

  # 多列合并过程，默认设置用于支持并合并
  # “instruction”、“input”、“output”数据集。要禁用，请将 multi_column_keys 设置为 []。
  #
  # 至少需要 2 列，且数据非空，才能进行合并。
  # 如果未找到匹配项，则回退到默认的 prompt/completion 或 text 列，
  # 如果未找到默认回退，则抛出错误。
  #
  # 重要提示：由于换行符通常用于 multi_column_suffix 等，
  #           你应使用双引号以确保此类值不会被转义。
  #           例如：multi_column_suffix: ["\n\n"]
  #
  # 请参阅：https://github.com/RWKV/RWKV-infctx-trainer/issues/34
  # 需要使用 "，否则换行符不会被正确标记化。
  # ---
  # multi_column_keys: ["instruction", "input", "output"]
  # multi_column_prefix: ["Instruction:\n", "Input:\n", "Output:\n"]
  # multi_column_suffix: ['', '', '']
  # multi_column_train_mask: [true, false, true]
  # multi_column_separator: "\n\n"
  
  # 对话合并过程。
  # 用于将完整对话数据集合并为单个文档。
  # 默认关闭（或将 conversation_key 设置为 []）。
  # conversation_formatting 目前支持 "iopairs" 或 "sender"。
  # ---
  # conversation_format: 'iopairs'
  # conversation_key: 'conversation'
  # conversation_end_of_conversation: "\n\nUser:"

  # Iopairs 特定配置。
  # 这意味着对话对象中的每个对象都是输入输出对。
  # 未来还将支持一种格式，其中一个键决定格式样式。
  # 如果 conversation_key 设置为 null，则使用根对象作为对话对象。
  # ---
  # conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
  # conversation_input_key_mask: {'input': false, 'output': true}
  # conversation_sender_suffix: {'input': "", 'output': ""}

  # Sender 特定配置。
  # 这意味着对话对象中的每个对象都是单个消息（带有发送者和消息键 - 或类似键）。
  # 输出由输入键映射决定，其余“sender_”配置由发送者键的值键控。
  # conversation_input_key_map: {'message': "\n\n{sender}: ", 'context': ''}
  # conversation_sender_key: 'sender'
  # conversation_sender_value_map: {'user': 'User', 'assistant': 'Assistant', 'system': 'System'}
  # conversation_sender_mask: {'user': false, 'assistant': true, 'system': false}
  # conversation_sender_suffix: {'user': "", 'assistant': "", 'system': ""}

  # 如果处理 prompt/completion jsonl 对，则默认屏蔽 prompt。
  # 使用此标志禁用此默认行为。
  # ---
  # disable_prompt_completion_mask: false

  # ----------------------------
  # 重新分块支持
  # ----------------------------

  # 文本数据集的重新分块，仅在 source 设置为 'text' 时完成，
  # 并将各种句子合并为更大的块，直到达到目标大小。
  #
  # 默认为 2048。
  #
  # 如果 source 未设置为 text（除非 text_rechunk_force），则忽略此设置。
  # 如果设置为零 / -1，则忽略此设置。
  # ---
  text_rechunk_size: 2048

  # 即使 source 不是 'text'，也应用文本重新分块。
  # 这仅在数据集过滤后完成，且 source 不是 'text'。
  # ---
  text_rechunk_force: False

  # 用于禁用文本文件的自动重新分块，如果设置为 false。
  # ---
  text_rechunk_auto: True

  # ----------------------------
  # 数据集打包支持
  # 建议用于混合文档大小的微调。
  # 对于基础模型“从头开始”，通常使用重新分块。
  # ----------------------------

  # 启用/禁用数据集打包的布尔标志。
  packing_enable: True

  # 用于确保此批次大小内的所有训练样本具有相同的长度。
  # 理想情况下，这应与你的实际“批次大小”完全一致。
  #
  # 使用 `8 * (3 * 4 * 5 * 6 * 7) = 20160` 作为默认值，因为它应与
  # 大量批次大小组合对齐。这有助于减少未对齐批次的数量，
  # 从而减少浪费的训练时间。
  packing_batchsize: 20160

  # 每个批次中对齐的分块大小，理想情况下应等于
  # 使用的训练上下文长度。
  packing_chunksize: 4096

  # 打包的最小大小，这应是 packing_chunksize 的倍数。
  # 默认为 -1，等于 packing_chunksize。
  packing_min_ctx_len: -1

  # 如果可能，按数据集顺序打包数据。
  # 这可以与 sort_by_length 一起使用，否则将进行洗牌。
  packing_in_sequence: False

  # ----------------------------
  # 特殊用例标志
  # ----------------------------

  # 在保存之前反转训练数据集顺序，这对于
  # 优化数据集打包过程很有用，当使用 packing_in_sequence
  # 和 sort_by_length 降序排序时。
  reverse_train_dataset_before_save: False


# 当前检查点路径，用于继续训练。
# 这应该是目录路径，并以 `.ckpt/` 结尾。
ckpt_path: null
